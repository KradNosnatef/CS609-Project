{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 3597,
     "status": "ok",
     "timestamp": 1698541545402,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "g-Alr2Rbfxbz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import random\n",
    "import copy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from array import array\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1698541545402,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "s5rQLv9beP6K"
   },
   "outputs": [],
   "source": [
    "class Car:\n",
    "    def __init__(self, tyre=\"Intermediate\"):\n",
    "        self.default_tyre = tyre\n",
    "        self.possible_tyres = [\"Ultrasoft\", \"Soft\", \"Intermediate\", \"Fullwet\"]\n",
    "        self.pitstop_time = 23\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.change_tyre(self.default_tyre)\n",
    "\n",
    "\n",
    "    def degrade(self, w, r):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            self.condition *= (1 - 0.0050*w - (2500-r)/90000)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            self.condition *= (1 - 0.0051*w - (2500-r)/93000)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            self.condition *= (1 - 0.0052*abs(0.5-w) - (2500-r)/95000)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            self.condition *= (1 - 0.0053*(1-w) - (2500-r)/97000)\n",
    "\n",
    "\n",
    "    def change_tyre(self, new_tyre):\n",
    "        assert new_tyre in self.possible_tyres\n",
    "        self.tyre = new_tyre\n",
    "        self.condition = 1.00\n",
    "\n",
    "\n",
    "    def get_velocity(self):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            vel = 80.7*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            vel = 80.1*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            vel = 79.5*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            vel = 79.0*(0.2 + 0.8*self.condition**1.5)\n",
    "        return vel\n",
    "\n",
    "\n",
    "class Track:\n",
    "    def __init__(self, car=Car()):\n",
    "        # self.radius and self.cur_weather are defined in self.reset()\n",
    "        self.total_laps = 162\n",
    "        self.car = car\n",
    "        self.possible_weather = [\"Dry\", \"20% Wet\", \"40% Wet\", \"60% Wet\", \"80% Wet\", \"100% Wet\"]\n",
    "        self.wetness = {\n",
    "            \"Dry\": 0.00, \"20% Wet\": 0.20, \"40% Wet\": 0.40, \"60% Wet\": 0.60, \"80% Wet\": 0.80, \"100% Wet\": 1.00\n",
    "        }\n",
    "        self.p_transition = {\n",
    "            \"Dry\": {\n",
    "                \"Dry\": 0.987, \"20% Wet\": 0.013, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"20% Wet\": {\n",
    "                \"Dry\": 0.012, \"20% Wet\": 0.975, \"40% Wet\": 0.013, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"40% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.012, \"40% Wet\": 0.975, \"60% Wet\": 0.013, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"60% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.012, \"60% Wet\": 0.975, \"80% Wet\": 0.013, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"80% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.012, \"80% Wet\": 0.975, \"100% Wet\": 0.013\n",
    "            },\n",
    "            \"100% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.012, \"100% Wet\": 0.988\n",
    "            }\n",
    "        }\n",
    "        self.num_actions = 5\n",
    "        self.actions = [a for a in range(self.num_actions)]\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.radius = np.random.randint(600,1201)\n",
    "        self.cur_weather = np.random.choice(self.possible_weather)\n",
    "        self.is_done = False\n",
    "        self.pitstop = False\n",
    "        self.laps_cleared = 0\n",
    "        self.car.reset()\n",
    "        return self._get_state()\n",
    "\n",
    "\n",
    "    def _get_state(self):\n",
    "        return [self.car.tyre, self.car.condition, self.cur_weather, self.radius, self.laps_cleared]\n",
    "\n",
    "\n",
    "    def transition(self, action=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int):\n",
    "                0. Make a pitstop and fit new ‘Ultrasoft’ tyres\n",
    "                1. Make a pitstop and fit new ‘Soft’ tyres\n",
    "                2. Make a pitstop and fit new ‘Intermediate’ tyres\n",
    "                3. Make a pitstop and fit new ‘Fullwet’ tyres\n",
    "                4. Continue the next lap without changing tyres\n",
    "        \"\"\"\n",
    "        ## Pitstop time will be added on the first eight of the subsequent lap\n",
    "        time_taken = 0\n",
    "        if self.laps_cleared == int(self.laps_cleared):\n",
    "            if self.pitstop:\n",
    "                self.car.change_tyre(self.committed_tyre)\n",
    "                time_taken += self.car.pitstop_time\n",
    "                self.pitstop = False\n",
    "\n",
    "        ## The environment is coded such that only an action taken at the start of the three-quarters mark of each lap matters\n",
    "        if self.laps_cleared - int(self.laps_cleared) == 0.75:\n",
    "            #print(self.laps_cleared)\n",
    "            if action < 4:\n",
    "                #print(action)\n",
    "                self.pitstop = True\n",
    "                self.committed_tyre = self.car.possible_tyres[action]\n",
    "            else:\n",
    "                self.pitstop = False\n",
    "\n",
    "        self.cur_weather = np.random.choice(\n",
    "            self.possible_weather, p=list(self.p_transition[self.cur_weather].values())\n",
    "        )\n",
    "        # we assume that degration happens only after a car has travelled the one-eighth lap\n",
    "        velocity = self.car.get_velocity()\n",
    "        time_taken += (2*np.pi*self.radius/8) / velocity\n",
    "        reward = 0 - time_taken\n",
    "        self.car.degrade(\n",
    "            w=self.wetness[self.cur_weather], r=self.radius\n",
    "        )\n",
    "        self.laps_cleared += 0.125\n",
    "\n",
    "        ##### Intro crash\n",
    "\n",
    "       # if self.cur_weather != \"Dry\":\n",
    "            ### calculate probability of crash\n",
    "\n",
    "            # p_crash = _______________\n",
    "        #    if np.random.rand() < p_crash:\n",
    "         #       self.is_done = True\n",
    "\n",
    "          #  else:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if self.laps_cleared == self.total_laps:\n",
    "            self.is_done = True\n",
    "\n",
    "        next_state = self._get_state()\n",
    "        return reward, next_state, self.is_done, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698541545402,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "tBo_5HF9mCxO"
   },
   "outputs": [],
   "source": [
    "def moving_average(values, window_size):\n",
    "    \"\"\"Calculate moving average with a given window size.\"\"\"\n",
    "    cumsum = np.cumsum(values)\n",
    "    cumsum[window_size:] = cumsum[window_size:] - cumsum[:-window_size]\n",
    "    return cumsum[window_size - 1:] / window_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698541545402,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "pG5Sh__0ZNED"
   },
   "outputs": [],
   "source": [
    "class Replay_buffer():\n",
    "    '''\n",
    "    Code based on:\n",
    "    https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py\n",
    "    Expects tuples of (state, next_state, action, reward, done)\n",
    "    '''\n",
    "    def __init__(self, max_size):\n",
    "        \"\"\"Create Replay buffer.\n",
    "        Parameters\n",
    "        ----------\n",
    "        size: int\n",
    "            Max number of transitions to store in the buffer. When the buffer\n",
    "            overflows the old memories are dropped.\n",
    "        \"\"\"\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def push(self, data):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = data\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(data)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "        Parameters\n",
    "        ----------\n",
    "        batch_size: int\n",
    "            How many transitions to sample.\n",
    "        Returns\n",
    "        -------\n",
    "        state: np.array\n",
    "            batch of state or observations\n",
    "        action: np.array\n",
    "            batch of actions executed given a state\n",
    "        reward: np.array\n",
    "            rewards received as results of executing action\n",
    "        next_state: np.array\n",
    "            next state next state or observations seen after executing action\n",
    "        done: np.array\n",
    "            done[i] = 1 if executing ation[i] resulted in\n",
    "            the end of an episode and 0 otherwise.\n",
    "        \"\"\"\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        state, next_state, action, reward, is_done = [], [], [], [], []\n",
    "\n",
    "        #replay_buffer.push((state, next_state, action, reward, is_done))\n",
    "\n",
    "\n",
    "        for i in ind:\n",
    "            st, n_st, act, rew, dn = self.storage[i]\n",
    "            state.append(st)\n",
    "            next_state.append(n_st)\n",
    "            action.append(np.array(act, copy=False))\n",
    "            reward.append(np.array(rew, copy=False))\n",
    "            is_done.append(np.array(dn, copy=False))\n",
    "\n",
    "        return state, next_state, np.array(action), np.array(reward).reshape(-1, 1), np.array(is_done).reshape(-1, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698541545402,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "je3BlmCvZV7O"
   },
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    \"\"\"\n",
    "    The Actor model takes in a state observation as input and\n",
    "    outputs an action, which is a continuous value.\n",
    "\n",
    "    It consists of four fully connected linear layers with ReLU activation functions and\n",
    "    a final output layer selects one single optimized action for the state\n",
    "    \"\"\"\n",
    "    def __init__(self, n_states, action_dim, hidden1):\n",
    "        super(Actor, self).__init__()\n",
    "        self.epsilon = 0.1\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, hidden1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden1, 5)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = self.net(state)\n",
    "        \n",
    "        softmax_output = torch.softmax(output, dim=1)\n",
    "        \n",
    "        #print(softmax_output)\n",
    "        \n",
    "        if np.random.uniform(0,1) < self.epsilon:\n",
    "            choice = torch.randint(low=0, high=5,size=())\n",
    "        else:\n",
    "            choice = torch.argmax(softmax_output)\n",
    "            \n",
    "        #print(\"choice is: \",choice,\" and softmax is: \",torch.argmax(softmax_output))\n",
    "        return choice\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    \"\"\"\n",
    "    The Critic model takes in both a state observation and an action as input and\n",
    "    outputs a Q-value, which estimates the expected total reward for the current state-action pair.\n",
    "\n",
    "    It consists of four linear layers with ReLU activation functions,\n",
    "    State and action inputs are concatenated before being fed into the first linear layer.\n",
    "\n",
    "    The output layer has a single output, representing the Q-value\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_states, action_dim, hidden2):\n",
    "        super(Critic, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_states + action_dim, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, hidden2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden2, action_dim)\n",
    "        )\n",
    "\n",
    "        #print(self.net)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        return self.net(torch.cat((state.reshape(1,5), action.reshape(1,1)),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1698541545402,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "INp5jVq2Zrle"
   },
   "outputs": [],
   "source": [
    "class OU_Noise(object):\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\n",
    "    code from :\n",
    "    https://math.stackexchange.com/questions/1287634/implementing-ornstein-uhlenbeck-in-matlab\n",
    "    The OU_Noise class has four attributes\n",
    "\n",
    "        size: the size of the noise vector to be generated\n",
    "        mu: the mean of the noise, set to 0 by default\n",
    "        theta: the rate of mean reversion, controlling how quickly the noise returns to the mean\n",
    "        sigma: the volatility of the noise, controlling the magnitude of fluctuations\n",
    "    \"\"\"\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Reset the internal state (= noise) to mean (mu).\"\"\"\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"Update internal state and return it as a noise sample.\n",
    "        This method uses the current state of the noise and generates the next sample\n",
    "        \"\"\"\n",
    "        dx = self.theta * (self.mu - self.state) + self.sigma * np.array([np.random.normal() for _ in range(len(self.state))])\n",
    "        self.state += dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1698541545402,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "UN6OLga4aVZT"
   },
   "outputs": [],
   "source": [
    "#Set Hyperparameters\n",
    "# Hyperparameters adapted for performance from\n",
    "#https://ai.stackexchange.com/questions/22945/ddpg-doesnt-converge-for-mountaincarcontinuous-v0-gym-environment\n",
    "capacity=10000\n",
    "batch_size=64\n",
    "update_iteration=100\n",
    "tau=0.001 # tau for soft updating\n",
    "gamma=0.99 # discount factor\n",
    "directory = './'\n",
    "hidden1=64 # hidden layer for actor\n",
    "hidden2=64 #hiiden laye for critic\n",
    "\n",
    "class DDPG(object):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        \"\"\"\n",
    "        Initializes the DDPG agent.\n",
    "        Takes three arguments:\n",
    "               state_dim which is the dimensionality of the state space,\n",
    "               action_dim which is the dimensionality of the action space, and\n",
    "               max_action which is the maximum value an action can take.\n",
    "\n",
    "        Creates a replay buffer, an actor-critic  networks and their corresponding target networks.\n",
    "        It also initializes the optimizer for both actor and critic networks alog with\n",
    "        counters to track the number of training iterations.\n",
    "        \"\"\"\n",
    "        self.replay_buffer = Replay_buffer(max_size=capacity)\n",
    "\n",
    "        self.actor = Actor(state_dim, action_dim, hidden1).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim,  hidden1).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=3e-3)\n",
    "\n",
    "        self.critic = Critic(state_dim, action_dim,  hidden2).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim,  hidden2).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=2e-2)\n",
    "        # learning rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        self.num_critic_update_iteration = 0\n",
    "        self.num_actor_update_iteration = 0\n",
    "        self.num_training = 0\n",
    "\n",
    "    def discretize_state(self, state):\n",
    "        tyre, condition, cur_weather, radius, laps_cleared = state\n",
    "\n",
    "        tyre_map = {\"Ultrasoft\":1,\n",
    "                     \"Soft\":2,\n",
    "                     \"Intermediate\":3,\n",
    "                     \"Fullwet\":4}\n",
    "\n",
    "        cur_weather_map = {\"Dry\":1,\n",
    "                    \"20% Wet\":2,\n",
    "                    \"40% Wet\":3,\n",
    "                    \"60% Wet\":4,\n",
    "                    \"80% Wet\":5,\n",
    "                    \"100% Wet\":6}\n",
    "\n",
    "        #print(np.array([tyre_map[tyre], condition, cur_weather_map[cur_weather], radius, laps_cleared]))\n",
    "\n",
    "        return (np.array([tyre_map[tyre], condition, cur_weather_map[cur_weather], radius, laps_cleared]))\n",
    "\n",
    "    def select_action(self, state):\n",
    "        #print(state)\n",
    "        \"\"\"\n",
    "        takes the current state as input and returns an action to take in that state.\n",
    "        It uses the actor network to map the state to an action.\n",
    "        \"\"\"\n",
    "        state = torch.FloatTensor(self.discretize_state(state).reshape(1, -1)).to(device)\n",
    "        #print(\"item:  \",round(self.actor(state).cpu().data.numpy().flatten()[0]))\n",
    "        return round(self.actor(state).cpu().data.numpy().flatten()[0])\n",
    "\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        updates the actor and critic networks using a batch of samples from the replay buffer.\n",
    "        For each sample in the batch, it computes the target Q value using the target critic network and the target actor network.\n",
    "        It then computes the current Q value\n",
    "        using the critic network and the action taken by the actor network.\n",
    "\n",
    "        It computes the critic loss as the mean squared error between the target Q value and the current Q value, and\n",
    "        updates the critic network using gradient descent.\n",
    "\n",
    "        It then computes the actor loss as the negative mean Q value using the critic network and the actor network, and\n",
    "        updates the actor network using gradient ascent.\n",
    "\n",
    "        Finally, it updates the target networks using\n",
    "        soft updates, where a small fraction of the actor and critic network weights are transferred to their target counterparts.\n",
    "        This process is repeated for a fixed number of iterations.\n",
    "        \"\"\"\n",
    "\n",
    "        for it in range(update_iteration):\n",
    "            # For each Sample in replay buffer batch\n",
    "\n",
    "            st, n_st, act, rew, is_d = self.replay_buffer.sample(batch_size)\n",
    "\n",
    "            for i in range(batch_size):\n",
    "                \n",
    "                #print(\"test: \",i,\" \",act[i])\n",
    "                state = torch.FloatTensor(self.discretize_state(st[i])).to(device)\n",
    "\n",
    "                action = torch.tensor(act[i]).to(device)\n",
    "                #print(i,\" \",act[i])\n",
    "                #print(action)\n",
    "                next_state = torch.FloatTensor(self.discretize_state(n_st[i])).to(device)\n",
    "                done = torch.FloatTensor(is_d[i]).to(device)\n",
    "                reward = torch.FloatTensor(rew[i]).to(device)\n",
    "\n",
    "                # Compute the target Q value\n",
    "                #print(\"HERE \",(torch.unsqueeze(next_state, 0)))\n",
    "                target_Q = self.critic_target(next_state, self.actor_target(torch.unsqueeze(next_state, 0)))\n",
    "\n",
    "                target_Q = reward + (done * gamma * target_Q).detach()\n",
    "\n",
    "                # Get current Q estimate\n",
    "                #print('ok 221')\n",
    "                #print(self.actor_target(next_state))\n",
    "                #print(action)\n",
    "                current_Q = self.critic(state, action)\n",
    "                #print('ok 222')\n",
    "                # Compute critic loss\n",
    "                critic_loss = F.mse_loss(current_Q, target_Q)\n",
    "\n",
    "                # Optimize the critic\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optimizer.step()\n",
    "                # Compute actor loss as the negative mean Q value using the critic network and the actor network\n",
    "                actor_loss = -self.critic(state, self.actor(torch.unsqueeze(state, 0))).mean()\n",
    "\n",
    "                  # Optimize the actor\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "\n",
    "\n",
    "            \"\"\"\n",
    "            Update the frozen target models using\n",
    "            soft updates, where\n",
    "            tau,a small fraction of the actor and critic network weights are transferred to their target counterparts.\n",
    "            \"\"\"\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "\n",
    "            self.num_actor_update_iteration += 1\n",
    "            self.num_critic_update_iteration += 1\n",
    "\n",
    "            #print(\"end of iter num: \",it)\n",
    "\n",
    "    def save(self):\n",
    "        \"\"\"\n",
    "        Saves the state dictionaries of the actor and critic networks to files\n",
    "        \"\"\"\n",
    "        torch.save(self.actor.state_dict(), directory + 'actor.pth')\n",
    "        torch.save(self.critic.state_dict(), directory + 'critic.pth')\n",
    "\n",
    "    def load(self):\n",
    "        \"\"\"\n",
    "        Loads the state dictionaries of the actor and critic networks to files\n",
    "        \"\"\"\n",
    "        self.actor.load_state_dict(torch.load(directory + 'actor.pth'))\n",
    "        self.critic.load_state_dict(torch.load(directory + 'critic.pth')) #Train the agent on a Mountain car using DDPG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1698541545402,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "NEaClpCVqvsU"
   },
   "outputs": [],
   "source": [
    "new_car = Car()\n",
    "env = Track()\n",
    "results = {}\n",
    "max_episode=100\n",
    "max_time_steps=5000\n",
    "max_action=5\n",
    "ep_r = 0\n",
    "total_step = 0\n",
    "rewards=[]\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 421761,
     "status": "ok",
     "timestamp": 1698544974870,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "FoMx1-H9aZmA",
    "outputId": "47a71498-6477-4892-ed5b-85a63f979645"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [35:12<00:00, 21.12s/it]\n"
     ]
    }
   ],
   "source": [
    "# Create a DDPG instance\n",
    "state_dim = 5\n",
    "action_dim = 1\n",
    "agent = DDPG(state_dim, action_dim)\n",
    "\n",
    "# Train the agent for max_episodes\n",
    "\n",
    "\n",
    "for i in tqdm(range(max_episode)):\n",
    "    G = 0\n",
    "    is_done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while not is_done:\n",
    "        #print(state)\n",
    "        action = agent.select_action(state)\n",
    "        # Add Gaussian noise to actions for exploration\n",
    "        #action = (action + np.random.normal(0, 1, size=action_dim)).clip(1, max_action)\n",
    "        #action += ou_noise.sample()\n",
    "\n",
    "        reward, next_state, is_done, _ = env.transition(action)\n",
    "\n",
    "        #reward, next_state, self.is_done, velocity\n",
    "\n",
    "        G += reward\n",
    "\n",
    "        agent.replay_buffer.push((state, next_state, action, reward, is_done))\n",
    "        #print(state)\n",
    "        state = next_state\n",
    "\n",
    "\n",
    "    rewards.append(G)\n",
    "    #total_step += step+1\n",
    "    #print(\"Episode: \\t{}  Total Reward: \\t{:0.2f}\".format( i, total_reward))\n",
    "    agent.update()\n",
    "    if i % 10 == 0:\n",
    "        agent.save()\n",
    "    #print(\"End of EP \",i)\n",
    "#env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1698544974879,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "rjI72bfRVx7x",
    "outputId": "8024f061-90fd-47d2-80ce-335a574a685e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-18925.28519367498,\n",
       " -15432.219126366757,\n",
       " -18061.681266956333,\n",
       " -14254.401022549195,\n",
       " -20024.968844123243,\n",
       " -18378.48871098118,\n",
       " -12938.826464198337,\n",
       " -17823.383201253546,\n",
       " -12183.565283357791,\n",
       " -13879.950006925996,\n",
       " -18911.604609284823,\n",
       " -17617.180527860706,\n",
       " -13905.861492924785,\n",
       " -13190.990752492851,\n",
       " -19268.42566810492,\n",
       " -12789.664687421979,\n",
       " -14428.683931419246,\n",
       " -17304.198020997712,\n",
       " -12682.681358497299,\n",
       " -14242.11217978202,\n",
       " -16045.817460669059,\n",
       " -14646.000485237837,\n",
       " -18182.097132590472,\n",
       " -17871.410078784927,\n",
       " -15649.647522448959,\n",
       " -19642.31599662163,\n",
       " -16636.492114480767,\n",
       " -15829.74430368892,\n",
       " -16996.684092184372,\n",
       " -12986.067168163145,\n",
       " -16389.716281513272,\n",
       " -12262.314751264483,\n",
       " -19779.69436014797,\n",
       " -19422.542954490193,\n",
       " -19075.847297252847,\n",
       " -15015.801554636022,\n",
       " -13432.638640273066,\n",
       " -18550.992215934966,\n",
       " -18818.727764982763,\n",
       " -19454.487485171525,\n",
       " -14205.855217018054,\n",
       " -16813.89595055127,\n",
       " -18030.23344253209,\n",
       " -12939.462578175095,\n",
       " -12504.474571274235,\n",
       " -18920.959332566996,\n",
       " -18307.88887892856,\n",
       " -15752.100518640582,\n",
       " -17046.220889434313,\n",
       " -18428.48905208138,\n",
       " -16188.962645472235,\n",
       " -16225.564568799677,\n",
       " -15737.368018508008,\n",
       " -19417.669609079254,\n",
       " -20021.011893017676,\n",
       " -18031.838089850113,\n",
       " -14168.000112081767,\n",
       " -15071.785165720954,\n",
       " -13025.025149492458,\n",
       " -15695.66003873915,\n",
       " -15497.24224090752,\n",
       " -15633.13582360927,\n",
       " -15955.600480330859,\n",
       " -19486.099271738236,\n",
       " -14262.046245458552,\n",
       " -17562.990241697105,\n",
       " -12362.188653396981,\n",
       " -14192.56278424122,\n",
       " -16336.106929158033,\n",
       " -19796.725665388178,\n",
       " -16774.162035897465,\n",
       " -19382.43780899141,\n",
       " -17546.884689845545,\n",
       " -18491.793029619334,\n",
       " -17010.251597650073,\n",
       " -15371.692662311734,\n",
       " -12078.589542941496,\n",
       " -13279.71021003223,\n",
       " -15644.314164661268,\n",
       " -13915.325343106095,\n",
       " -19388.000351139894,\n",
       " -14134.778964977351,\n",
       " -14039.572209325268,\n",
       " -16047.073419096283,\n",
       " -17161.48363203107,\n",
       " -14057.778130787754,\n",
       " -13958.155690020112,\n",
       " -19292.804451671444,\n",
       " -17490.73138575202,\n",
       " -18513.264656288247,\n",
       " -15175.550335356951,\n",
       " -14264.096129407322,\n",
       " -13343.731742270751,\n",
       " -15799.297369823253,\n",
       " -19031.55308247234,\n",
       " -18011.64026574808,\n",
       " -19874.761786736475,\n",
       " -14327.869287970258,\n",
       " -15038.304254309358,\n",
       " -17160.95858871449]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "executionInfo": {
     "elapsed": 40,
     "status": "error",
     "timestamp": 1698485974291,
     "user": {
      "displayName": "Chuming Yip",
      "userId": "01418243307543943804"
     },
     "user_tz": -480
    },
    "id": "peqWd3nrav5O",
    "outputId": "d99cdfb1-1772-4458-81dc-ee61ad465438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode \t0, the episode reward is \t-14807.30\n",
      "radius is : 799\n",
      "Episode \t1, the episode reward is \t-18012.91\n",
      "radius is : 1034\n",
      "Episode \t2, the episode reward is \t-12440.03\n",
      "radius is : 625\n",
      "Episode \t3, the episode reward is \t-16390.67\n",
      "radius is : 921\n",
      "Episode \t4, the episode reward is \t-13516.56\n",
      "radius is : 702\n",
      "Episode \t5, the episode reward is \t-19865.36\n",
      "radius is : 1179\n",
      "Episode \t6, the episode reward is \t-20001.17\n",
      "radius is : 1196\n",
      "Episode \t7, the episode reward is \t-12047.91\n",
      "radius is : 600\n",
      "Episode \t8, the episode reward is \t-15406.79\n",
      "radius is : 844\n",
      "Episode \t9, the episode reward is \t-17714.05\n",
      "radius is : 1023\n",
      "Episode \t10, the episode reward is \t-17583.21\n",
      "radius is : 1004\n",
      "Episode \t11, the episode reward is \t-16803.63\n",
      "radius is : 954\n",
      "Episode \t12, the episode reward is \t-13913.04\n",
      "radius is : 732\n",
      "Episode \t13, the episode reward is \t-19626.58\n",
      "radius is : 1175\n",
      "Episode \t14, the episode reward is \t-15351.94\n",
      "radius is : 840\n",
      "Episode \t15, the episode reward is \t-15119.69\n",
      "radius is : 832\n",
      "Episode \t16, the episode reward is \t-18802.90\n",
      "radius is : 1106\n",
      "Episode \t17, the episode reward is \t-13918.46\n",
      "radius is : 731\n",
      "Episode \t18, the episode reward is \t-17041.12\n",
      "radius is : 976\n",
      "Episode \t19, the episode reward is \t-14882.47\n",
      "radius is : 804\n",
      "Episode \t20, the episode reward is \t-14916.29\n",
      "radius is : 807\n",
      "Episode \t21, the episode reward is \t-18055.41\n",
      "radius is : 1044\n",
      "Episode \t22, the episode reward is \t-19807.87\n",
      "radius is : 1189\n",
      "Episode \t23, the episode reward is \t-19208.67\n",
      "radius is : 1132\n",
      "Episode \t24, the episode reward is \t-16307.22\n",
      "radius is : 910\n",
      "Episode \t25, the episode reward is \t-14682.48\n",
      "radius is : 796\n",
      "Episode \t26, the episode reward is \t-14330.71\n",
      "radius is : 761\n",
      "Episode \t27, the episode reward is \t-17664.07\n",
      "radius is : 1009\n",
      "Episode \t28, the episode reward is \t-19830.19\n",
      "radius is : 1176\n",
      "Episode \t29, the episode reward is \t-14112.30\n",
      "radius is : 746\n",
      "Episode \t30, the episode reward is \t-18712.47\n",
      "radius is : 1107\n",
      "Episode \t31, the episode reward is \t-14279.99\n",
      "radius is : 758\n",
      "Episode \t32, the episode reward is \t-19500.57\n",
      "radius is : 1163\n",
      "Episode \t33, the episode reward is \t-16658.84\n",
      "radius is : 934\n",
      "Episode \t34, the episode reward is \t-14477.98\n",
      "radius is : 776\n",
      "Episode \t35, the episode reward is \t-18435.95\n",
      "radius is : 1082\n",
      "Episode \t36, the episode reward is \t-19760.54\n",
      "radius is : 1173\n",
      "Episode \t37, the episode reward is \t-17910.10\n",
      "radius is : 1035\n",
      "Episode \t38, the episode reward is \t-13554.09\n",
      "radius is : 710\n",
      "Episode \t39, the episode reward is \t-13493.46\n",
      "radius is : 711\n",
      "Episode \t40, the episode reward is \t-13319.58\n",
      "radius is : 693\n",
      "Episode \t41, the episode reward is \t-15661.41\n",
      "radius is : 869\n",
      "Episode \t42, the episode reward is \t-18296.73\n",
      "radius is : 1069\n",
      "Episode \t43, the episode reward is \t-17878.46\n",
      "radius is : 1030\n",
      "Episode \t44, the episode reward is \t-15776.31\n",
      "radius is : 874\n",
      "Episode \t45, the episode reward is \t-19341.62\n",
      "radius is : 1138\n",
      "Episode \t46, the episode reward is \t-18317.35\n",
      "radius is : 1075\n",
      "Episode \t47, the episode reward is \t-14190.17\n",
      "radius is : 752\n",
      "Episode \t48, the episode reward is \t-16645.91\n",
      "radius is : 932\n",
      "Episode \t49, the episode reward is \t-13460.17\n",
      "radius is : 697\n",
      "Episode \t50, the episode reward is \t-14231.98\n",
      "radius is : 757\n",
      "Episode \t51, the episode reward is \t-13543.98\n",
      "radius is : 716\n",
      "Episode \t52, the episode reward is \t-12687.31\n",
      "radius is : 642\n",
      "Episode \t53, the episode reward is \t-12320.51\n",
      "radius is : 620\n",
      "Episode \t54, the episode reward is \t-19327.42\n",
      "radius is : 1143\n",
      "Episode \t55, the episode reward is \t-16889.53\n",
      "radius is : 957\n",
      "Episode \t56, the episode reward is \t-12164.88\n",
      "radius is : 606\n",
      "Episode \t57, the episode reward is \t-19870.29\n",
      "radius is : 1180\n",
      "Episode \t58, the episode reward is \t-19117.04\n",
      "radius is : 1128\n",
      "Episode \t59, the episode reward is \t-13901.86\n",
      "radius is : 735\n",
      "Episode \t60, the episode reward is \t-13894.20\n",
      "radius is : 737\n",
      "Episode \t61, the episode reward is \t-15953.60\n",
      "radius is : 889\n",
      "Episode \t62, the episode reward is \t-13631.97\n",
      "radius is : 715\n",
      "Episode \t63, the episode reward is \t-14115.55\n",
      "radius is : 747\n",
      "Episode \t64, the episode reward is \t-14444.64\n",
      "radius is : 773\n",
      "Episode \t65, the episode reward is \t-16828.47\n",
      "radius is : 954\n",
      "Episode \t66, the episode reward is \t-18761.39\n",
      "radius is : 1101\n",
      "Episode \t67, the episode reward is \t-17072.60\n",
      "radius is : 974\n",
      "Episode \t68, the episode reward is \t-15722.38\n",
      "radius is : 875\n",
      "Episode \t69, the episode reward is \t-13192.66\n",
      "radius is : 688\n",
      "Episode \t70, the episode reward is \t-18549.04\n",
      "radius is : 1082\n",
      "Episode \t71, the episode reward is \t-13221.00\n",
      "radius is : 682\n",
      "Episode \t72, the episode reward is \t-17247.84\n",
      "radius is : 984\n",
      "Episode \t73, the episode reward is \t-16268.89\n",
      "radius is : 913\n",
      "Episode \t74, the episode reward is \t-16680.83\n",
      "radius is : 942\n",
      "Episode \t75, the episode reward is \t-15683.06\n",
      "radius is : 862\n",
      "Episode \t76, the episode reward is \t-19114.64\n",
      "radius is : 1118\n",
      "Episode \t77, the episode reward is \t-18049.28\n",
      "radius is : 1052\n",
      "Episode \t78, the episode reward is \t-19893.58\n",
      "radius is : 1180\n",
      "Episode \t79, the episode reward is \t-18933.36\n",
      "radius is : 1121\n",
      "Episode \t80, the episode reward is \t-14030.96\n",
      "radius is : 744\n",
      "Episode \t81, the episode reward is \t-12807.18\n",
      "radius is : 656\n",
      "Episode \t82, the episode reward is \t-19031.43\n",
      "radius is : 1114\n",
      "Episode \t83, the episode reward is \t-15267.86\n",
      "radius is : 835\n",
      "Episode \t84, the episode reward is \t-12320.77\n",
      "radius is : 614\n",
      "Episode \t85, the episode reward is \t-13903.80\n",
      "radius is : 735\n",
      "Episode \t86, the episode reward is \t-18327.69\n",
      "radius is : 1069\n",
      "Episode \t87, the episode reward is \t-15344.66\n",
      "radius is : 842\n",
      "Episode \t88, the episode reward is \t-18913.56\n",
      "radius is : 1106\n",
      "Episode \t89, the episode reward is \t-13719.99\n",
      "radius is : 717\n",
      "Episode \t90, the episode reward is \t-17127.72\n",
      "radius is : 970\n",
      "Episode \t91, the episode reward is \t-14208.19\n",
      "radius is : 760\n",
      "Episode \t92, the episode reward is \t-12206.86\n",
      "radius is : 609\n",
      "Episode \t93, the episode reward is \t-18032.95\n",
      "radius is : 1041\n",
      "Episode \t94, the episode reward is \t-15244.25\n",
      "radius is : 830\n",
      "Episode \t95, the episode reward is \t-12214.79\n",
      "radius is : 608\n",
      "Episode \t96, the episode reward is \t-13935.00\n",
      "radius is : 733\n",
      "Episode \t97, the episode reward is \t-14154.59\n",
      "radius is : 748\n",
      "Episode \t98, the episode reward is \t-19833.00\n",
      "radius is : 1189\n",
      "Episode \t99, the episode reward is \t-12352.25\n",
      "radius is : 625\n"
     ]
    }
   ],
   "source": [
    "test_iteration=100\n",
    "\n",
    "for i in (range(test_iteration)):\n",
    "    G = 0\n",
    "    is_done = False\n",
    "    state = env.reset()\n",
    "\n",
    "    while not is_done:\n",
    "        action = agent.select_action(state)\n",
    "        reward, next_state, is_done, _ = env.transition(action)\n",
    "        G += reward\n",
    "        if is_done:\n",
    "            print(\"Episode \\t{}, the episode reward is \\t{:0.2f}\".format(i, G))\n",
    "            print(\"radius is :\",state[3])\n",
    "            G = 0\n",
    "            break\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIH8RVs86D5f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPXRvcIF55phKg0SGwI4Q96",
   "gpuType": "V100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
