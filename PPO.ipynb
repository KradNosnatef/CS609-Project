{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2f7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c509d033",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508088e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, out_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float)\n",
    "        \n",
    "        activation1 = F.relu(self.layer1(obs))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2274e",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c4b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,obs_dim,act_dim,env):\n",
    "        self._init_hyperparameters()\n",
    "\n",
    "        self.env=env\n",
    "        self.obs_dim=obs_dim\n",
    "        self.act_dim=act_dim\n",
    "\n",
    "        self.actor=FeedForwardNN(self.obs_dim,self.act_dim)\n",
    "        self.critic=FeedForwardNN(self.obs_dim,1)\n",
    "\n",
    "        self.actor_optim=Adam(self.actor.parameters(),lr=self.lr)\n",
    "        self.critic_optim=Adam(self.critic.parameters(),lr=self.lr)\n",
    "\n",
    "        self.cov_var=torch.full(size=(self.act_dim,),fill_value=0.5)\n",
    "        self.cov_mat=torch.diag(self.cov_var)\n",
    "\n",
    "    \n",
    "    def learn(self,total_timesteps):\n",
    "        t_so_far=0\n",
    "        while t_so_far<total_timesteps:\n",
    "            batch_obs,batch_acts,batch_log_probs,batch_rtgs,batch_lens=self.rollout()\n",
    "\n",
    "            t_so_far+=np.sum(batch_lens)\n",
    "\n",
    "            V,_=self.evaluate(batch_obs,batch_acts)\n",
    "\n",
    "            A_k=batch_rtgs-V.detach()\n",
    "\n",
    "            A_k=(A_k-A_k.mean())/(A_k.std()+1e-10)\n",
    "\n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                V,curr_log_probs=self.evaluate(batch_obs,batch_acts)\n",
    "                \n",
    "                ratios=torch.exp(curr_log_probs-batch_log_probs)\n",
    "\n",
    "                surr1=ratios*A_k\n",
    "                surr2=torch.clamp(ratios,1-self.clip,1+self.clip)*A_k\n",
    "\n",
    "                actor_loss=(-torch.min(surr1,surr2)).mean()\n",
    "                critic_loss=nn.MSELoss()(V,batch_rtgs)\n",
    "\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                self.actor_optim.step()\n",
    "\n",
    "                self.critic_optim.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                self.critic_optim.step()\n",
    "\n",
    "    def evaluate(self,batch_obs,batch_acts):\n",
    "        V=self.critic(batch_obs).squeeze()\n",
    "\n",
    "        mean=self.actor(batch_obs)\n",
    "        dist=MultivariateNormal(mean,self.cov_mat)\n",
    "        log_probs=dist.log_prob(batch_acts)\n",
    "\n",
    "        return V,log_probs\n",
    "            \n",
    "    def _init_hyperparameters(self):\n",
    "        self.timesteps_per_batch=24000\n",
    "        self.max_timesteps_per_episode=1600\n",
    "        self.gamma=0.995\n",
    "        self.n_updates_per_iteration=5\n",
    "        self.clip=0.002\n",
    "        self.lr=0.0005\n",
    "\n",
    "        self.set=None\n",
    "\n",
    "    def rollout(self):\n",
    "        # Batch data\n",
    "        batch_obs = []             # batch observations\n",
    "        batch_acts = []            # batch actions\n",
    "        batch_log_probs = []       # log probs of each action\n",
    "        batch_rews = []            # batch rewards\n",
    "        batch_rtgs = []            # batch rewards-to-go\n",
    "        batch_lens = []            # episodic lengths in batch\n",
    "\n",
    "        t=0\n",
    "        while t<self.timesteps_per_batch:\n",
    "            ep_rews=[]\n",
    "            obs=self.env.reset()\n",
    "            done=False\n",
    "\n",
    "            G=0\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                t+=1\n",
    "                batch_obs.append(obs)\n",
    "                action,log_prob=self.get_action(obs)\n",
    "\n",
    "                ngz_p=[i-min(action) for i in action]\n",
    "                #print(\"emmmm:{}\".format(ngz_p))\n",
    "                sampled_action=random.choices([0,1,2,3,4],weights=ngz_p,k=1)\n",
    "                #print(\"     why:{}\".format(sampled_action))\n",
    "\n",
    "                reward,obs,done,_=self.env.transition(sampled_action[0])\n",
    "                G+=reward\n",
    "\n",
    "                ep_rews.append(reward)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "\n",
    "                if done:\n",
    "                    print(\"done,G=%.2f\" % G)\n",
    "                    break\n",
    "            \n",
    "            batch_lens.append(ep_t+1)\n",
    "            batch_rews.append(ep_rews)\n",
    "        \n",
    "        batch_obs=torch.tensor(batch_obs,dtype=torch.float32)\n",
    "        batch_acts=torch.tensor(batch_acts,dtype=torch.float32)\n",
    "        batch_log_probs=torch.tensor(batch_log_probs,dtype=torch.float32)\n",
    "\n",
    "        batch_rtgs=self.compute_rtgs(batch_rews)\n",
    "\n",
    "        return batch_obs,batch_acts,batch_log_probs,batch_rtgs,batch_lens\n",
    "    \n",
    "    def get_action(self,obs):\n",
    "        mean=self.actor(torch.tensor(obs,dtype=torch.float32))\n",
    "        dist=MultivariateNormal(mean,self.cov_mat)\n",
    "        action=dist.sample()\n",
    "        log_prob=dist.log_prob(action)\n",
    "        return action.detach().numpy(),log_prob.detach()\n",
    "    \n",
    "    def compute_rtgs(self,batch_rews):\n",
    "        batch_rtgs=[]\n",
    "\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "            discounted_reward=0\n",
    "            for reward in reversed(ep_rews):\n",
    "                discounted_reward=reward+discounted_reward*self.gamma\n",
    "                batch_rtgs.insert(0,discounted_reward)\n",
    "        \n",
    "        batch_rtgs=torch.tensor(batch_rtgs,dtype=torch.float32)\n",
    "        return batch_rtgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19cd767",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5f3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,n_actions,n_observations):\n",
    "        pass\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Simple-minded agent that always select action 1\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341de44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringStateToIntState(state):\n",
    "    tyreDict={}\n",
    "    tyreDict[\"Ultrasoft\"]=0\n",
    "    tyreDict[\"Soft\"]=1\n",
    "    tyreDict[\"Intermediate\"]=2\n",
    "    tyreDict[\"Fullwet\"]=3\n",
    "\n",
    "    weatherDict={}\n",
    "    weatherDict[\"Dry\"]=0\n",
    "    weatherDict[\"20% Wet\"]=1\n",
    "    weatherDict[\"40% Wet\"]=2\n",
    "    weatherDict[\"60% Wet\"]=3\n",
    "    weatherDict[\"80% Wet\"]=4\n",
    "    weatherDict[\"100% Wet\"]=5\n",
    "\n",
    "    return([tyreDict[state[0]],state[1],weatherDict[state[2]],state[3],state[4]])\n",
    "\n",
    "\n",
    "\n",
    "class Car:\n",
    "    def __init__(self, tyre=\"Intermediate\"):\n",
    "        self.default_tyre = tyre\n",
    "        self.possible_tyres = [\"Ultrasoft\", \"Soft\", \"Intermediate\", \"Fullwet\"]\n",
    "        self.pitstop_time = 23\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.change_tyre(self.default_tyre)\n",
    "    \n",
    "    \n",
    "    def degrade(self, w, r):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            self.condition *= (1 - 0.0050*w - (2500-r)/90000)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            self.condition *= (1 - 0.0051*w - (2500-r)/93000)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            self.condition *= (1 - 0.0052*abs(0.5-w) - (2500-r)/95000)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            self.condition *= (1 - 0.0053*(1-w) - (2500-r)/97000)\n",
    "        \n",
    "        \n",
    "    def change_tyre(self, new_tyre):\n",
    "        assert new_tyre in self.possible_tyres\n",
    "        self.tyre = new_tyre\n",
    "        self.condition = 1.00\n",
    "    \n",
    "    \n",
    "    def get_velocity(self):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            vel = 80.7*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            vel = 80.1*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            vel = 79.5*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            vel = 79.0*(0.2 + 0.8*self.condition**1.5)\n",
    "        return vel\n",
    "\n",
    "    \n",
    "class Track:\n",
    "    def __init__(self, car=Car()):\n",
    "        # self.radius and self.cur_weather are defined in self.reset()\n",
    "        self.total_laps = 162\n",
    "        self.car = car\n",
    "        self.possible_weather = [\"Dry\", \"20% Wet\", \"40% Wet\", \"60% Wet\", \"80% Wet\", \"100% Wet\"]\n",
    "        self.wetness = {\n",
    "            \"Dry\": 0.00, \"20% Wet\": 0.20, \"40% Wet\": 0.40, \"60% Wet\": 0.60, \"80% Wet\": 0.80, \"100% Wet\": 1.00\n",
    "        }\n",
    "        self.p_transition = {\n",
    "            \"Dry\": {\n",
    "                \"Dry\": 0.987, \"20% Wet\": 0.013, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"20% Wet\": {\n",
    "                \"Dry\": 0.012, \"20% Wet\": 0.975, \"40% Wet\": 0.013, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"40% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.012, \"40% Wet\": 0.975, \"60% Wet\": 0.013, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"60% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.012, \"60% Wet\": 0.975, \"80% Wet\": 0.013, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"80% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.012, \"80% Wet\": 0.975, \"100% Wet\": 0.013\n",
    "            },\n",
    "            \"100% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.012, \"100% Wet\": 0.988\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.radius = 1200#np.random.randint(600,1201)\n",
    "        self.cur_weather = np.random.choice(self.possible_weather)\n",
    "        self.is_done = False\n",
    "        self.pitstop = False\n",
    "        self.laps_cleared = 0\n",
    "        self.car.reset()\n",
    "        return stringStateToIntState(self._get_state())\n",
    "    \n",
    "    \n",
    "    def _get_state(self):\n",
    "        return [self.car.tyre, self.car.condition, self.cur_weather, self.radius, self.laps_cleared]\n",
    "        \n",
    "    \n",
    "    def transition(self, action=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int):\n",
    "                0. Make a pitstop and fit new ‘Ultrasoft’ tyres\n",
    "                1. Make a pitstop and fit new ‘Soft’ tyres\n",
    "                2. Make a pitstop and fit new ‘Intermediate’ tyres\n",
    "                3. Make a pitstop and fit new ‘Fullwet’ tyres\n",
    "                4. Continue the next lap without changing tyres\n",
    "        \"\"\"\n",
    "        ## Pitstop time will be added on the first eight of the subsequent lap\n",
    "        time_taken = 0\n",
    "        if self.laps_cleared == int(self.laps_cleared):\n",
    "            if self.pitstop:\n",
    "                self.car.change_tyre(self.committed_tyre)\n",
    "                time_taken += self.car.pitstop_time\n",
    "                self.pitstop = False\n",
    "        \n",
    "        ## The environment is coded such that only an action taken at the start of the three-quarters mark of each lap matters\n",
    "        if self.laps_cleared - int(self.laps_cleared) == 0.75:\n",
    "            if action < 4:\n",
    "                self.pitstop = True\n",
    "                self.committed_tyre = self.car.possible_tyres[action]\n",
    "            else:\n",
    "                self.pitstop = False\n",
    "        \n",
    "        self.cur_weather = np.random.choice(\n",
    "            self.possible_weather, p=list(self.p_transition[self.cur_weather].values())\n",
    "        )\n",
    "        # we assume that degration happens only after a car has travelled the one-eighth lap\n",
    "        velocity = self.car.get_velocity()\n",
    "        time_taken += (2*np.pi*self.radius/8) / velocity\n",
    "        reward = 0-time_taken\n",
    "        #reward=0-action\n",
    "        self.car.degrade(\n",
    "            w=self.wetness[self.cur_weather], r=self.radius\n",
    "        )\n",
    "        self.laps_cleared += 0.125\n",
    "        \n",
    "        if self.laps_cleared == self.total_laps:\n",
    "            self.is_done = True\n",
    "        \n",
    "        next_state = stringStateToIntState(self._get_state())\n",
    "        return reward, next_state, self.is_done, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9bc5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_car = Car()\n",
    "env = Track(new_car)\n",
    "\n",
    "agent = Agent(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc6d6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -19977\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()    \n",
    "done = False\n",
    "G = 0\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    reward, next_state, done, velocity = env.transition(action)\n",
    "    # added velocity for sanity check\n",
    "    state = deepcopy(next_state)\n",
    "    G += reward\n",
    "\n",
    "print(\"G: %d\" % G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3670d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done,G=-19903.26\n",
      "done,G=-19938.18\n",
      "done,G=-20056.45\n",
      "done,G=-19954.44\n",
      "done,G=-20429.96\n",
      "done,G=-19938.55\n",
      "done,G=-19880.90\n",
      "done,G=-19965.88\n",
      "done,G=-20038.94\n",
      "done,G=-20084.08\n",
      "done,G=-20026.11\n",
      "done,G=-19963.78\n",
      "done,G=-20037.14\n",
      "done,G=-19990.54\n",
      "done,G=-19907.28\n",
      "done,G=-20051.19\n",
      "done,G=-20087.23\n",
      "done,G=-20091.22\n",
      "done,G=-20122.48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8571/1441262019.py:111: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  batch_acts=torch.tensor(batch_acts,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done,G=-20103.71\n",
      "done,G=-20355.33\n",
      "done,G=-20052.86\n",
      "done,G=-20167.28\n",
      "done,G=-19721.47\n",
      "done,G=-19941.18\n",
      "done,G=-20095.28\n",
      "done,G=-20131.86\n",
      "done,G=-20319.11\n",
      "done,G=-19889.54\n",
      "done,G=-20237.18\n",
      "done,G=-20136.60\n",
      "done,G=-20967.25\n",
      "done,G=-20138.50\n",
      "done,G=-20368.21\n",
      "done,G=-20103.05\n",
      "done,G=-20356.54\n",
      "done,G=-20262.58\n",
      "done,G=-20404.75\n",
      "done,G=-20064.06\n",
      "done,G=-19996.18\n",
      "done,G=-20300.28\n",
      "done,G=-20205.56\n",
      "done,G=-20218.16\n",
      "done,G=-20308.77\n",
      "done,G=-19894.91\n",
      "done,G=-19939.07\n",
      "done,G=-20492.58\n",
      "done,G=-19950.94\n",
      "done,G=-20089.08\n",
      "done,G=-20157.05\n",
      "done,G=-19994.19\n",
      "done,G=-19862.59\n",
      "done,G=-20001.58\n",
      "done,G=-20021.12\n",
      "done,G=-20017.43\n",
      "done,G=-20054.86\n",
      "done,G=-20391.88\n",
      "done,G=-20119.85\n",
      "done,G=-20198.30\n",
      "done,G=-20212.62\n",
      "done,G=-19829.68\n",
      "done,G=-20517.43\n",
      "done,G=-19860.08\n",
      "done,G=-20073.98\n",
      "done,G=-20011.48\n",
      "done,G=-20150.22\n",
      "done,G=-20426.80\n",
      "done,G=-20541.72\n",
      "done,G=-20037.54\n",
      "done,G=-19998.89\n",
      "done,G=-20032.82\n",
      "done,G=-19901.10\n",
      "done,G=-20128.09\n",
      "done,G=-20052.92\n",
      "done,G=-19795.91\n",
      "done,G=-20304.18\n",
      "done,G=-19980.64\n",
      "done,G=-20216.32\n",
      "done,G=-20244.25\n",
      "done,G=-20102.89\n",
      "done,G=-19953.28\n",
      "done,G=-20123.77\n",
      "done,G=-20403.48\n",
      "done,G=-19848.46\n",
      "done,G=-20382.68\n",
      "done,G=-20135.28\n",
      "done,G=-20488.13\n",
      "done,G=-20420.42\n",
      "done,G=-20330.90\n",
      "done,G=-20061.11\n",
      "done,G=-19850.06\n",
      "done,G=-20403.29\n",
      "done,G=-19899.45\n",
      "done,G=-19953.87\n",
      "done,G=-20209.14\n",
      "done,G=-19923.36\n",
      "done,G=-20033.85\n",
      "done,G=-20066.78\n",
      "done,G=-20209.35\n",
      "done,G=-20097.88\n",
      "done,G=-20260.61\n",
      "done,G=-20034.24\n",
      "done,G=-20389.05\n",
      "done,G=-20188.68\n",
      "done,G=-20107.51\n",
      "done,G=-20141.14\n",
      "done,G=-19837.02\n",
      "done,G=-20505.73\n",
      "done,G=-20227.22\n",
      "done,G=-20259.11\n",
      "done,G=-20090.97\n",
      "done,G=-20168.11\n",
      "done,G=-19854.99\n",
      "done,G=-20002.99\n",
      "done,G=-20162.88\n",
      "done,G=-19922.62\n",
      "done,G=-20233.08\n",
      "done,G=-20163.95\n",
      "done,G=-20374.82\n",
      "done,G=-19957.11\n",
      "done,G=-20300.32\n",
      "done,G=-19966.69\n",
      "done,G=-19868.17\n",
      "done,G=-20150.66\n",
      "done,G=-20086.35\n",
      "done,G=-19836.27\n",
      "done,G=-20062.99\n",
      "done,G=-20584.48\n",
      "done,G=-20044.87\n",
      "done,G=-20246.65\n",
      "done,G=-20032.48\n",
      "done,G=-20262.97\n",
      "done,G=-19777.38\n",
      "done,G=-20094.07\n",
      "done,G=-19871.71\n",
      "done,G=-20203.19\n",
      "done,G=-20108.50\n",
      "done,G=-20134.21\n",
      "done,G=-20171.05\n",
      "done,G=-20225.29\n",
      "done,G=-20016.96\n",
      "done,G=-20449.45\n",
      "done,G=-20403.38\n",
      "done,G=-19965.43\n",
      "done,G=-20139.10\n",
      "done,G=-20289.78\n",
      "done,G=-20132.11\n",
      "done,G=-20448.35\n",
      "done,G=-20275.62\n",
      "done,G=-19999.82\n",
      "done,G=-20651.82\n",
      "done,G=-20056.95\n",
      "done,G=-20210.22\n",
      "done,G=-20102.17\n",
      "done,G=-20180.02\n",
      "done,G=-20272.64\n",
      "done,G=-20124.20\n",
      "done,G=-19835.09\n",
      "done,G=-20185.46\n",
      "done,G=-20119.25\n",
      "done,G=-20098.52\n",
      "done,G=-19862.06\n",
      "done,G=-20052.26\n",
      "done,G=-20319.17\n",
      "done,G=-19855.63\n",
      "done,G=-19879.75\n",
      "done,G=-20256.44\n",
      "done,G=-20274.91\n",
      "done,G=-20108.06\n",
      "done,G=-20251.47\n",
      "done,G=-19932.82\n",
      "done,G=-20054.56\n",
      "done,G=-20072.15\n",
      "done,G=-20196.48\n",
      "done,G=-20141.15\n",
      "done,G=-20148.95\n",
      "done,G=-19942.96\n",
      "done,G=-20129.51\n",
      "done,G=-20436.93\n",
      "done,G=-20327.96\n",
      "done,G=-20168.47\n",
      "done,G=-20067.34\n",
      "done,G=-19961.80\n",
      "done,G=-20145.85\n",
      "done,G=-20024.58\n",
      "done,G=-20156.82\n",
      "done,G=-20014.49\n",
      "done,G=-20326.30\n",
      "done,G=-20199.33\n",
      "done,G=-20133.40\n",
      "done,G=-20040.92\n",
      "done,G=-20160.82\n",
      "done,G=-20168.97\n",
      "done,G=-20213.62\n",
      "done,G=-20214.58\n",
      "done,G=-20214.40\n",
      "done,G=-20516.26\n",
      "done,G=-19794.84\n",
      "done,G=-19974.76\n",
      "done,G=-20231.78\n",
      "done,G=-20094.10\n",
      "done,G=-20008.10\n",
      "done,G=-20819.20\n",
      "done,G=-19955.95\n",
      "done,G=-20222.62\n",
      "done,G=-19884.56\n",
      "done,G=-20489.91\n",
      "done,G=-20176.67\n",
      "done,G=-20031.40\n",
      "done,G=-20259.10\n",
      "done,G=-20152.77\n",
      "done,G=-20075.73\n",
      "done,G=-20310.25\n",
      "done,G=-20326.66\n",
      "done,G=-19846.17\n",
      "done,G=-20552.69\n",
      "done,G=-20225.19\n",
      "done,G=-20471.11\n",
      "done,G=-20151.50\n",
      "done,G=-20266.80\n",
      "done,G=-20383.07\n",
      "done,G=-20624.89\n",
      "done,G=-20092.62\n",
      "done,G=-20177.24\n",
      "done,G=-20461.09\n",
      "done,G=-20298.72\n",
      "done,G=-19873.63\n",
      "done,G=-20177.06\n",
      "done,G=-20082.24\n",
      "done,G=-19926.05\n",
      "done,G=-20119.47\n",
      "done,G=-20107.41\n",
      "done,G=-20200.44\n",
      "done,G=-20034.36\n",
      "done,G=-20622.01\n",
      "done,G=-19828.79\n",
      "done,G=-20295.64\n",
      "done,G=-19968.13\n",
      "done,G=-19963.85\n",
      "done,G=-20054.27\n",
      "done,G=-20234.31\n",
      "done,G=-20180.27\n",
      "done,G=-20470.67\n",
      "done,G=-20130.46\n",
      "done,G=-20081.42\n"
     ]
    }
   ],
   "source": [
    "agent=PPO(5,5,env)\n",
    "agent.learn(100000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
