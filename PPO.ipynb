{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d2f7a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.optim import Adam\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c509d033",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508088e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardNN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(FeedForwardNN, self).__init__()\n",
    "        self.layer1 = nn.Linear(in_dim, 64)\n",
    "        self.layer2 = nn.Linear(64, 64)\n",
    "        self.layer3 = nn.Linear(64, out_dim)\n",
    "\n",
    "    def forward(self, obs):\n",
    "        # Convert observation to tensor if it's a numpy array\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs,device=device, dtype=torch.float)\n",
    "        \n",
    "        activation1 = F.relu(self.layer1(obs))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        output = self.layer3(activation2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c2274e",
   "metadata": {},
   "source": [
    "## PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c4b62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self,obs_dim,act_dim,env):\n",
    "        self._init_hyperparameters()\n",
    "\n",
    "        self.env=env\n",
    "        self.obs_dim=obs_dim\n",
    "        self.act_dim=act_dim\n",
    "\n",
    "        self.actor=FeedForwardNN(self.obs_dim,self.act_dim).to(device)\n",
    "        self.critic=FeedForwardNN(self.obs_dim,1).to(device)\n",
    "\n",
    "        self.actor_optim=Adam(self.actor.parameters(),lr=self.lr)\n",
    "        self.critic_optim=Adam(self.critic.parameters(),lr=self.lr)\n",
    "\n",
    "        self.cov_var=torch.full(size=(self.act_dim,),device=device,fill_value=0.25)\n",
    "        self.cov_mat=torch.diag(self.cov_var)\n",
    "\n",
    "        self.max_grad_norm=0.5\n",
    "\n",
    "    \n",
    "    def learn(self,total_timesteps):\n",
    "        t_so_far=0\n",
    "        while t_so_far<total_timesteps:\n",
    "            batch_obs,batch_acts,batch_log_probs,batch_rtgs,batch_lens=self.rollout()\n",
    "\n",
    "            t_so_far+=np.sum(batch_lens)\n",
    "\n",
    "            V,_=self.evaluate(batch_obs,batch_acts)\n",
    "\n",
    "            A_k=batch_rtgs-V.detach()\n",
    "\n",
    "            A_k=(A_k-A_k.mean())/(A_k.std()+1e-10)\n",
    "\n",
    "            for _ in range(self.n_updates_per_iteration):\n",
    "                V,curr_log_probs=self.evaluate(batch_obs,batch_acts)\n",
    "                \n",
    "                ratios=torch.exp(curr_log_probs-batch_log_probs)\n",
    "\n",
    "                surr1=ratios*A_k\n",
    "                surr2=torch.clamp(ratios,1-self.clip,1+self.clip)*A_k\n",
    "\n",
    "                actor_loss=(-torch.min(surr1,surr2)).mean()\n",
    "                critic_loss=nn.MSELoss()(V,batch_rtgs)\n",
    "\n",
    "                self.actor_optim.zero_grad()\n",
    "                actor_loss.backward(retain_graph=True)\n",
    "                nn.utils.clip_grad_norm_(self.actor.parameters(), self.max_grad_norm) \n",
    "                self.actor_optim.step()\n",
    "\n",
    "                self.critic_optim.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                nn.utils.clip_grad_norm_(self.critic.parameters(), self.max_grad_norm)\n",
    "                self.critic_optim.step()\n",
    "\n",
    "    def evaluate(self,batch_obs,batch_acts):\n",
    "        V=self.critic(batch_obs).squeeze()\n",
    "\n",
    "        mean=self.actor(batch_obs)\n",
    "        dist=MultivariateNormal(mean,self.cov_mat)\n",
    "        log_probs=dist.log_prob(batch_acts)\n",
    "\n",
    "        return V,log_probs\n",
    "            \n",
    "    def _init_hyperparameters(self):\n",
    "        self.timesteps_per_batch=9000\n",
    "        self.max_timesteps_per_episode=3200\n",
    "        self.gamma=0.98\n",
    "        self.n_updates_per_iteration=2\n",
    "        self.clip=0.02\n",
    "        self.lr=0.01\n",
    "\n",
    "        self.set=None\n",
    "\n",
    "    def rollout(self):\n",
    "        # Batch data\n",
    "        batch_obs = []             # batch observations\n",
    "        batch_acts = []            # batch actions\n",
    "        batch_log_probs = []       # log probs of each action\n",
    "        batch_rews = []            # batch rewards\n",
    "        batch_rtgs = []            # batch rewards-to-go\n",
    "        batch_lens = []            # episodic lengths in batch\n",
    "\n",
    "        t=0\n",
    "        while t<self.timesteps_per_batch:\n",
    "            ep_rews=[]\n",
    "            obs=self.env.reset()\n",
    "            done=False\n",
    "\n",
    "            G=0\n",
    "            for ep_t in range(self.max_timesteps_per_episode):\n",
    "                t+=1\n",
    "                batch_obs.append(obs)\n",
    "                action,log_prob=self.get_action(obs)\n",
    "\n",
    "                ngz_p=[i-min(action) for i in action]\n",
    "                #print(\"emmmm:{}\".format(ngz_p))\n",
    "                sampled_action=random.choices([0,1,2,3,4],weights=ngz_p,k=1)\n",
    "                #print(\"     why:{}\".format(sampled_action))\n",
    "                #sampled_action=[0]\n",
    "\n",
    "                reward,obs,done,_=self.env.transition(sampled_action[0])\n",
    "                G+=reward\n",
    "\n",
    "                ep_rews.append(reward)\n",
    "                batch_acts.append(action)\n",
    "                batch_log_probs.append(log_prob)\n",
    "\n",
    "                if done:\n",
    "                    print(\"done,G=%.2f\" % G)\n",
    "                    break\n",
    "            \n",
    "            batch_lens.append(ep_t+1)\n",
    "            batch_rews.append(ep_rews)\n",
    "        print(\"batch over, avg reward={:.2f}\".format(np.mean(batch_rews)))\n",
    "        \n",
    "        batch_obs=torch.tensor(batch_obs,device=device,dtype=torch.float32)\n",
    "        batch_acts=torch.tensor(batch_acts,device=device,dtype=torch.float32)\n",
    "        batch_log_probs=torch.tensor(batch_log_probs,device=device,dtype=torch.float32)\n",
    "\n",
    "        batch_rtgs=self.compute_rtgs(batch_rews)\n",
    "\n",
    "        return batch_obs,batch_acts,batch_log_probs,batch_rtgs,batch_lens\n",
    "    \n",
    "    def get_action(self,obs):\n",
    "        mean=self.actor(torch.tensor(obs,device=device,dtype=torch.float32))\n",
    "        dist=MultivariateNormal(mean,self.cov_mat)\n",
    "        action=dist.sample()\n",
    "        log_prob=dist.log_prob(action)\n",
    "        return torch.Tensor.cpu(action.detach()).numpy(),log_prob.detach()\n",
    "    \n",
    "    def compute_rtgs(self,batch_rews):\n",
    "        batch_rtgs=[]\n",
    "\n",
    "        for ep_rews in reversed(batch_rews):\n",
    "            discounted_reward=0\n",
    "            for reward in reversed(ep_rews):\n",
    "                discounted_reward=reward+discounted_reward*self.gamma\n",
    "                batch_rtgs.insert(0,discounted_reward)\n",
    "        \n",
    "        batch_rtgs=torch.tensor(batch_rtgs,device=device,dtype=torch.float32)\n",
    "        return batch_rtgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19cd767",
   "metadata": {},
   "source": [
    "## Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb5f3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,n_actions,n_observations):\n",
    "        pass\n",
    "    \n",
    "    def act(self, state):\n",
    "        # Simple-minded agent that always select action 1\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "341de44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringStateToIntState(state):\n",
    "    tyreDict={}\n",
    "    tyreDict[\"Ultrasoft\"]=0\n",
    "    tyreDict[\"Soft\"]=1\n",
    "    tyreDict[\"Intermediate\"]=2\n",
    "    tyreDict[\"Fullwet\"]=3\n",
    "\n",
    "    weatherDict={}\n",
    "    weatherDict[\"Dry\"]=0\n",
    "    weatherDict[\"20% Wet\"]=1\n",
    "    weatherDict[\"40% Wet\"]=2\n",
    "    weatherDict[\"60% Wet\"]=3\n",
    "    weatherDict[\"80% Wet\"]=4\n",
    "    weatherDict[\"100% Wet\"]=5\n",
    "\n",
    "    return([tyreDict[state[0]],state[1],weatherDict[state[2]],state[3],state[4]])\n",
    "\n",
    "\n",
    "\n",
    "class Car:\n",
    "    def __init__(self, tyre=\"Intermediate\"):\n",
    "        self.default_tyre = tyre\n",
    "        self.possible_tyres = [\"Ultrasoft\", \"Soft\", \"Intermediate\", \"Fullwet\"]\n",
    "        self.pitstop_time = 23\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.change_tyre(self.default_tyre)\n",
    "    \n",
    "    \n",
    "    def degrade(self, w, r):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            self.condition *= (1 - 0.0050*w - (2500-r)/90000)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            self.condition *= (1 - 0.0051*w - (2500-r)/93000)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            self.condition *= (1 - 0.0052*abs(0.5-w) - (2500-r)/95000)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            self.condition *= (1 - 0.0053*(1-w) - (2500-r)/97000)\n",
    "        \n",
    "        \n",
    "    def change_tyre(self, new_tyre):\n",
    "        assert new_tyre in self.possible_tyres\n",
    "        self.tyre = new_tyre\n",
    "        self.condition = 1.00\n",
    "    \n",
    "    \n",
    "    def get_velocity(self):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            vel = 80.7*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            vel = 80.1*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            vel = 79.5*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            vel = 79.0*(0.2 + 0.8*self.condition**1.5)\n",
    "        return vel\n",
    "\n",
    "    \n",
    "class Track:\n",
    "    def __init__(self, car=Car()):\n",
    "        # self.radius and self.cur_weather are defined in self.reset()\n",
    "        self.total_laps = 162\n",
    "        self.car = car\n",
    "        self.possible_weather = [\"Dry\", \"20% Wet\", \"40% Wet\", \"60% Wet\", \"80% Wet\", \"100% Wet\"]\n",
    "        self.wetness = {\n",
    "            \"Dry\": 0.00, \"20% Wet\": 0.20, \"40% Wet\": 0.40, \"60% Wet\": 0.60, \"80% Wet\": 0.80, \"100% Wet\": 1.00\n",
    "        }\n",
    "        self.p_transition = {\n",
    "            \"Dry\": {\n",
    "                \"Dry\": 0.987, \"20% Wet\": 0.013, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"20% Wet\": {\n",
    "                \"Dry\": 0.012, \"20% Wet\": 0.975, \"40% Wet\": 0.013, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"40% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.012, \"40% Wet\": 0.975, \"60% Wet\": 0.013, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"60% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.012, \"60% Wet\": 0.975, \"80% Wet\": 0.013, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"80% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.012, \"80% Wet\": 0.975, \"100% Wet\": 0.013\n",
    "            },\n",
    "            \"100% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.012, \"100% Wet\": 0.988\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.radius = 1200#np.random.randint(600,1201)\n",
    "        self.cur_weather = np.random.choice(self.possible_weather)\n",
    "        self.is_done = False\n",
    "        self.pitstop = False\n",
    "        self.laps_cleared = 0\n",
    "        self.car.reset()\n",
    "        return stringStateToIntState(self._get_state())\n",
    "    \n",
    "    \n",
    "    def _get_state(self):\n",
    "        return [self.car.tyre, self.car.condition, self.cur_weather, self.radius, self.laps_cleared]\n",
    "        \n",
    "    \n",
    "    def transition(self, action=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int):\n",
    "                0. Make a pitstop and fit new ‘Ultrasoft’ tyres\n",
    "                1. Make a pitstop and fit new ‘Soft’ tyres\n",
    "                2. Make a pitstop and fit new ‘Intermediate’ tyres\n",
    "                3. Make a pitstop and fit new ‘Fullwet’ tyres\n",
    "                4. Continue the next lap without changing tyres\n",
    "        \"\"\"\n",
    "        ## Pitstop time will be added on the first eight of the subsequent lap\n",
    "        time_taken = 0\n",
    "        if self.laps_cleared == int(self.laps_cleared):\n",
    "            if self.pitstop:\n",
    "                self.car.change_tyre(self.committed_tyre)\n",
    "                time_taken += self.car.pitstop_time\n",
    "                self.pitstop = False\n",
    "        \n",
    "        ## The environment is coded such that only an action taken at the start of the three-quarters mark of each lap matters\n",
    "        if self.laps_cleared - int(self.laps_cleared) == 0.75:\n",
    "            if action < 4:\n",
    "                self.pitstop = True\n",
    "                self.committed_tyre = self.car.possible_tyres[action]\n",
    "            else:\n",
    "                self.pitstop = False\n",
    "        \n",
    "        self.cur_weather = np.random.choice(\n",
    "            self.possible_weather, p=list(self.p_transition[self.cur_weather].values())\n",
    "        )\n",
    "        # we assume that degration happens only after a car has travelled the one-eighth lap\n",
    "        velocity = self.car.get_velocity()\n",
    "        time_taken += (2*np.pi*self.radius/8) / velocity\n",
    "        reward = 0-time_taken\n",
    "        #reward=0-action\n",
    "        self.car.degrade(\n",
    "            w=self.wetness[self.cur_weather], r=self.radius\n",
    "        )\n",
    "        self.laps_cleared += 0.125\n",
    "        \n",
    "        if self.laps_cleared == self.total_laps:\n",
    "            self.is_done = True\n",
    "        \n",
    "        next_state = stringStateToIntState(self._get_state())\n",
    "        return reward, next_state, self.is_done, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9bc5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_car = Car()\n",
    "env = Track(new_car)\n",
    "\n",
    "agent = Agent(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddc6d6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -20095\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()    \n",
    "done = False\n",
    "G = 0\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    reward, next_state, done, velocity = env.transition(action)\n",
    "    # added velocity for sanity check\n",
    "    state = deepcopy(next_state)\n",
    "    G += reward\n",
    "\n",
    "print(\"G: %d\" % G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb3670d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done,G=-20041.12\n",
      "done,G=-19980.79\n",
      "done,G=-19951.57\n",
      "done,G=-19732.77\n",
      "done,G=-19957.73\n",
      "done,G=-20032.73\n",
      "done,G=-19944.08\n",
      "batch over, avg reward=-15.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62060/789756256.py:117: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  batch_acts=torch.tensor(batch_acts,device=device,dtype=torch.float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done,G=-20018.63\n",
      "done,G=-20103.92\n",
      "done,G=-20003.76\n",
      "done,G=-20112.34\n",
      "done,G=-20131.96\n",
      "done,G=-19985.95\n",
      "done,G=-20060.40\n",
      "batch over, avg reward=-15.48\n",
      "done,G=-19972.06\n",
      "done,G=-20009.44\n",
      "done,G=-20026.56\n",
      "done,G=-20120.42\n",
      "done,G=-20061.34\n",
      "done,G=-20000.44\n",
      "done,G=-20040.28\n",
      "batch over, avg reward=-15.46\n",
      "done,G=-20088.54\n",
      "done,G=-20019.81\n",
      "done,G=-20049.35\n",
      "done,G=-20055.50\n",
      "done,G=-20076.80\n",
      "done,G=-20057.18\n",
      "done,G=-20046.60\n",
      "batch over, avg reward=-15.48\n",
      "done,G=-19990.96\n",
      "done,G=-20048.43\n",
      "done,G=-20038.50\n",
      "done,G=-20021.39\n",
      "done,G=-20069.74\n",
      "done,G=-20073.48\n",
      "done,G=-19987.93\n",
      "batch over, avg reward=-15.46\n",
      "done,G=-20070.26\n",
      "done,G=-20100.55\n",
      "done,G=-20031.45\n",
      "done,G=-20010.32\n",
      "done,G=-19974.06\n",
      "done,G=-20024.09\n",
      "done,G=-20018.20\n",
      "batch over, avg reward=-15.46\n",
      "done,G=-20006.70\n",
      "done,G=-20026.11\n",
      "done,G=-20045.70\n"
     ]
    }
   ],
   "source": [
    "agent=PPO(5,5,env)\n",
    "agent.learn(100000000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
