{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312ce7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple,deque\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a721ebf",
   "metadata": {},
   "source": [
    "### DQN Decision Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508088e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 1024)\n",
    "        self.layer2 = nn.Linear(1024, 256)\n",
    "        self.layer3 = nn.Linear(256, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5f3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,n_actions,n_observations):\n",
    "        self.BATCH_SIZE=128\n",
    "        self.BATCH_SIZE = 128\n",
    "        self.GAMMA = 0.99\n",
    "        self.EPS_START = 0.99\n",
    "        self.EPS_END = 0.05\n",
    "        self.EPS_DECAY = 100\n",
    "        self.TAU = 0.005\n",
    "        self.LR = 1e-4\n",
    "\n",
    "        self.n_actions=n_actions\n",
    "        self.n_observations=n_observations\n",
    "\n",
    "        self.policy_net=DQN(self.n_observations,self.n_actions).to(device)\n",
    "        self.target_net = DQN(self.n_observations, self.n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.optimizer=optim.AdamW(self.policy_net.parameters(),lr=self.LR,amsgrad=True)\n",
    "        self.memory=ReplayMemory(10000)\n",
    "        self.steps_done=0\n",
    "        self.episode_G=[]\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def select_action(self,state):\n",
    "        sample=random.random()\n",
    "        eps_threshold=self.EPS_END+(self.EPS_START-self.EPS_END)*math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.steps_done+=1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randint(0,4)]], device=device, dtype=torch.long)\n",
    "\n",
    "    def plot_durations(self,show_result=False):\n",
    "        plt.figure(1)\n",
    "        durations_t = torch.tensor(self.episode_G, dtype=torch.float)\n",
    "        if show_result:\n",
    "            plt.title('Result')\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title('Training...')\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('G')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        # Take 100 episode averages and plot them too\n",
    "        '''if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            plt.plot(means.numpy())\n",
    "'''\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                display.display(plt.gcf())\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory)<self.BATCH_SIZE:\n",
    "            return\n",
    "        transitions=self.memory.sample(self.BATCH_SIZE)\n",
    "        batch=Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask=torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)),device=device,dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        state_action_values=self.policy_net(state_batch).gather(1,action_batch)\n",
    "\n",
    "        next_state_values=torch.zeros(self.BATCH_SIZE,device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "        criterion=nn.SmoothL1Loss()\n",
    "        loss=criterion(state_action_values,expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def act(self, state):\n",
    "        # Simple-minded agent that always select action 1\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341de44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringStateToIntState(state):\n",
    "    tyreDict={}\n",
    "    tyreDict[\"Ultrasoft\"]=0\n",
    "    tyreDict[\"Soft\"]=1\n",
    "    tyreDict[\"Intermediate\"]=2\n",
    "    tyreDict[\"Fullwet\"]=3\n",
    "\n",
    "    weatherDict={}\n",
    "    weatherDict[\"Dry\"]=0\n",
    "    weatherDict[\"20% Wet\"]=1\n",
    "    weatherDict[\"40% Wet\"]=2\n",
    "    weatherDict[\"60% Wet\"]=3\n",
    "    weatherDict[\"80% Wet\"]=4\n",
    "    weatherDict[\"100% Wet\"]=5\n",
    "\n",
    "    return([tyreDict[state[0]],state[1],weatherDict[state[2]],state[3],state[4]])\n",
    "\n",
    "\n",
    "\n",
    "class Car:\n",
    "    def __init__(self, tyre=\"Intermediate\"):\n",
    "        self.default_tyre = tyre\n",
    "        self.possible_tyres = [\"Ultrasoft\", \"Soft\", \"Intermediate\", \"Fullwet\"]\n",
    "        self.pitstop_time = 23\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.change_tyre(self.default_tyre)\n",
    "    \n",
    "    \n",
    "    def degrade(self, w, r):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            self.condition *= (1 - 0.0050*w - (2500-r)/90000)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            self.condition *= (1 - 0.0051*w - (2500-r)/93000)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            self.condition *= (1 - 0.0052*abs(0.5-w) - (2500-r)/95000)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            self.condition *= (1 - 0.0053*(1-w) - (2500-r)/97000)\n",
    "        \n",
    "        \n",
    "    def change_tyre(self, new_tyre):\n",
    "        assert new_tyre in self.possible_tyres\n",
    "        self.tyre = new_tyre\n",
    "        self.condition = 1.00\n",
    "    \n",
    "    \n",
    "    def get_velocity(self):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            vel = 80.7*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            vel = 80.1*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            vel = 79.5*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            vel = 79.0*(0.2 + 0.8*self.condition**1.5)\n",
    "        return vel\n",
    "\n",
    "    \n",
    "class Track:\n",
    "    def __init__(self, car=Car()):\n",
    "        # self.radius and self.cur_weather are defined in self.reset()\n",
    "        self.total_laps = 3\n",
    "        self.car = car\n",
    "        self.possible_weather = [\"Dry\", \"20% Wet\", \"40% Wet\", \"60% Wet\", \"80% Wet\", \"100% Wet\"]\n",
    "        self.wetness = {\n",
    "            \"Dry\": 0.00, \"20% Wet\": 0.20, \"40% Wet\": 0.40, \"60% Wet\": 0.60, \"80% Wet\": 0.80, \"100% Wet\": 1.00\n",
    "        }\n",
    "        self.p_transition = {\n",
    "            \"Dry\": {\n",
    "                \"Dry\": 0.987, \"20% Wet\": 0.013, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"20% Wet\": {\n",
    "                \"Dry\": 0.012, \"20% Wet\": 0.975, \"40% Wet\": 0.013, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"40% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.012, \"40% Wet\": 0.975, \"60% Wet\": 0.013, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"60% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.012, \"60% Wet\": 0.975, \"80% Wet\": 0.013, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"80% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.012, \"80% Wet\": 0.975, \"100% Wet\": 0.013\n",
    "            },\n",
    "            \"100% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.012, \"100% Wet\": 0.988\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.radius = np.random.randint(600,1201)\n",
    "        self.cur_weather = np.random.choice(self.possible_weather)\n",
    "        self.is_done = False\n",
    "        self.pitstop = False\n",
    "        self.laps_cleared = 0\n",
    "        self.car.reset()\n",
    "        return self._get_state()\n",
    "    \n",
    "    \n",
    "    def _get_state(self):\n",
    "        return [self.car.tyre, self.car.condition, self.cur_weather, self.radius, self.laps_cleared]\n",
    "        \n",
    "    \n",
    "    def transition(self, action=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int):\n",
    "                0. Make a pitstop and fit new ‘Ultrasoft’ tyres\n",
    "                1. Make a pitstop and fit new ‘Soft’ tyres\n",
    "                2. Make a pitstop and fit new ‘Intermediate’ tyres\n",
    "                3. Make a pitstop and fit new ‘Fullwet’ tyres\n",
    "                4. Continue the next lap without changing tyres\n",
    "        \"\"\"\n",
    "        ## Pitstop time will be added on the first eight of the subsequent lap\n",
    "        time_taken = 0\n",
    "        if self.laps_cleared == int(self.laps_cleared):\n",
    "            if self.pitstop:\n",
    "                self.car.change_tyre(self.committed_tyre)\n",
    "                time_taken += self.car.pitstop_time\n",
    "                self.pitstop = False\n",
    "        \n",
    "        ## The environment is coded such that only an action taken at the start of the three-quarters mark of each lap matters\n",
    "        if self.laps_cleared - int(self.laps_cleared) == 0.75:\n",
    "            if action < 4:\n",
    "                self.pitstop = True\n",
    "                self.committed_tyre = self.car.possible_tyres[action]\n",
    "            else:\n",
    "                self.pitstop = False\n",
    "        \n",
    "        self.cur_weather = np.random.choice(\n",
    "            self.possible_weather, p=list(self.p_transition[self.cur_weather].values())\n",
    "        )\n",
    "        # we assume that degration happens only after a car has travelled the one-eighth lap\n",
    "        velocity = self.car.get_velocity()\n",
    "        time_taken += (2*np.pi*self.radius/8) / velocity\n",
    "        reward = 0-time_taken\n",
    "        self.car.degrade(\n",
    "            w=self.wetness[self.cur_weather], r=self.radius\n",
    "        )\n",
    "        self.laps_cleared += 0.125\n",
    "        \n",
    "        if self.laps_cleared == self.total_laps:\n",
    "            self.is_done = True\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return reward, next_state, self.is_done, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9bc5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_car = Car()\n",
    "env = Track(new_car)\n",
    "\n",
    "agent = Agent(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc6d6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: -345\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()    \n",
    "done = False\n",
    "G = 0\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    reward, next_state, done, velocity = env.transition(action)\n",
    "    # added velocity for sanity check\n",
    "    state = deepcopy(next_state)\n",
    "    G += reward\n",
    "\n",
    "print(\"G: %d\" % G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90067d6c",
   "metadata": {},
   "source": [
    "### Race 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c74f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store historical weather\n",
    "holder = []\n",
    "\n",
    "state = env.reset()\n",
    "start_state = deepcopy(state)\n",
    "done = False\n",
    "G = 0\n",
    "while not done:\n",
    "    holder.append(env.cur_weather)\n",
    "    action = agent.act(state)\n",
    "    reward, next_state, done, velocity = env.transition(action)\n",
    "    # added velocity for sanity check\n",
    "    state = deepcopy(next_state)\n",
    "    G += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e572da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = Agent(5,5)\n",
    "group2 = Agent(5,5)\n",
    "group3 = Agent(5,5)\n",
    "group4 = Agent(5,5)\n",
    "group5 = Agent(5,5)\n",
    "group6 = Agent(5,5)\n",
    "group7 = Agent(5,5)\n",
    "group8 = Agent(5,5)\n",
    "group9 = Agent(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46100663",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/fustcianshan/repo/CS609-Project/DQN.ipynb 单元格 11\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m state \u001b[39m=\u001b[39m deepcopy(next_state)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=38'>39</a>\u001b[0m state_tensor\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mtensor(stringStateToIntState(state),dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32,device\u001b[39m=\u001b[39mdevice)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=40'>41</a>\u001b[0m agent\u001b[39m.\u001b[39;49moptimize_model()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=41'>42</a>\u001b[0m target_net_state_dict \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtarget_net\u001b[39m.\u001b[39mstate_dict()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m policy_net_state_dict \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mpolicy_net\u001b[39m.\u001b[39mstate_dict()\n",
      "\u001b[1;32m/home/fustcianshan/repo/CS609-Project/DQN.ipynb 单元格 11\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=88'>89</a>\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=90'>91</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_value_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_net\u001b[39m.\u001b[39mparameters(), \u001b[39m100\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:373\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    369\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    370\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    371\u001b[0m             )\n\u001b[0;32m--> 373\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    374\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    376\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[39m.\u001b[39m_dynamo\u001b[39m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:184\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    171\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m\"\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    173\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[1;32m    174\u001b[0m         group,\n\u001b[1;32m    175\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    181\u001b[0m         state_steps,\n\u001b[1;32m    182\u001b[0m     )\n\u001b[0;32m--> 184\u001b[0m     adamw(\n\u001b[1;32m    185\u001b[0m         params_with_grad,\n\u001b[1;32m    186\u001b[0m         grads,\n\u001b[1;32m    187\u001b[0m         exp_avgs,\n\u001b[1;32m    188\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    189\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    190\u001b[0m         state_steps,\n\u001b[1;32m    191\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    192\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    193\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    194\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    195\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    196\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    197\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    198\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    199\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    200\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    201\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m\"\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m    202\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    203\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m    204\u001b[0m     )\n\u001b[1;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:335\u001b[0m, in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    333\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adamw\n\u001b[0;32m--> 335\u001b[0m func(\n\u001b[1;32m    336\u001b[0m     params,\n\u001b[1;32m    337\u001b[0m     grads,\n\u001b[1;32m    338\u001b[0m     exp_avgs,\n\u001b[1;32m    339\u001b[0m     exp_avg_sqs,\n\u001b[1;32m    340\u001b[0m     max_exp_avg_sqs,\n\u001b[1;32m    341\u001b[0m     state_steps,\n\u001b[1;32m    342\u001b[0m     amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[1;32m    343\u001b[0m     beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[1;32m    344\u001b[0m     beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[1;32m    345\u001b[0m     lr\u001b[39m=\u001b[39;49mlr,\n\u001b[1;32m    346\u001b[0m     weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[1;32m    347\u001b[0m     eps\u001b[39m=\u001b[39;49meps,\n\u001b[1;32m    348\u001b[0m     maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[1;32m    349\u001b[0m     capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[1;32m    350\u001b[0m     differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[1;32m    351\u001b[0m     grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[1;32m    352\u001b[0m     found_inf\u001b[39m=\u001b[39;49mfound_inf,\n\u001b[1;32m    353\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adamw.py:542\u001b[0m, in \u001b[0;36m_multi_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    540\u001b[0m torch\u001b[39m.\u001b[39m_foreach_lerp_(device_exp_avgs, device_grads, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m--> 542\u001b[0m torch\u001b[39m.\u001b[39;49m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[1;32m    543\u001b[0m torch\u001b[39m.\u001b[39m_foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[1;32m    545\u001b[0m \u001b[39m# Delete the local intermediate since it won't be used anymore to save on peak memory\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes=1000\n",
    "\n",
    "agent=Agent(5,5)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "        start_weather, radius = start_state[2], start_state[3]\n",
    "\n",
    "    #for agent in [group1]:#, group2, group3, group4, group5, group6, group7, group8, group9]:\n",
    "        state = env.reset() \n",
    "        state_tensor=torch.tensor(stringStateToIntState(state),dtype=torch.float32,device=device).unsqueeze(0)\n",
    "        env.cur_weather = start_weather   # assert common start weather\n",
    "        env.radius = radius               # assert common track radius\n",
    "        done = False\n",
    "        G = 0\n",
    "        i = 0\n",
    "        t=0\n",
    "        while True:\n",
    "            env.cur_weather = holder[i]   # assert weather transition\n",
    "\n",
    "            action_tensor=agent.select_action(state_tensor)\n",
    "\n",
    "            reward,next_state,terminated,velocity=env.transition(action_tensor.item())\n",
    "\n",
    "            reward_tensor=torch.tensor([reward],device=device)\n",
    "            done=terminated\n",
    "\n",
    "            # added velocity for sanity check\n",
    "            G += reward\n",
    "            i += 1\n",
    "\n",
    "            if terminated:\n",
    "                next_state_tensor=None\n",
    "            else:\n",
    "                next_state_tensor=torch.tensor(stringStateToIntState(next_state),dtype=torch.float32,device=device).unsqueeze(0)\n",
    "\n",
    "            agent.memory.push(state_tensor,action_tensor,next_state_tensor,reward_tensor)\n",
    "\n",
    "            state = deepcopy(next_state)\n",
    "            state_tensor=torch.tensor(stringStateToIntState(state),dtype=torch.float32,device=device).unsqueeze(0)\n",
    "\n",
    "            agent.optimize_model()\n",
    "            target_net_state_dict = agent.target_net.state_dict()\n",
    "            policy_net_state_dict = agent.policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*agent.TAU + target_net_state_dict[key]*(1-agent.TAU)\n",
    "            agent.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done:\n",
    "                agent.episode_G.append(G)\n",
    "                agent.plot_durations()\n",
    "                break\n",
    "            \n",
    "        \n",
    "        #print(\"G: %.2f\" % G)\n",
    "\n",
    "print('Complete')\n",
    "group1.plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b44ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
