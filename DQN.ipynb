{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "312ce7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import namedtuple,deque\n",
    "from itertools import count\n",
    "import random\n",
    "import math\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a721ebf",
   "metadata": {},
   "source": [
    "### DQN Decision Maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "508088e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, n_actions)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb5f3d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,n_actions,n_observations):\n",
    "        self.BATCH_SIZE = 1024\n",
    "        self.GAMMA = 0.99\n",
    "        self.EPS_START = 0.9\n",
    "        self.EPS_END = 0.01\n",
    "        self.EPS_DECAY = 2000\n",
    "        self.TAU = 0.005\n",
    "        self.LR = 1e-4\n",
    "        self.currentEPS=0\n",
    "\n",
    "        self.n_actions=n_actions\n",
    "        self.n_observations=n_observations\n",
    "\n",
    "        self.policy_net=DQN(self.n_observations,self.n_actions).to(device)\n",
    "        self.target_net = DQN(self.n_observations, self.n_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "        self.optimizer=optim.AdamW(self.policy_net.parameters(),lr=self.LR,amsgrad=True)\n",
    "        self.memory=ReplayMemory(10000)\n",
    "        self.steps_done=0\n",
    "        self.episode_G=[]\n",
    "\n",
    "        pass\n",
    "    \n",
    "    def select_action(self,state):\n",
    "        sample=random.random()\n",
    "        eps_threshold=self.EPS_END+(self.EPS_START-self.EPS_END)*math.exp(-1. * self.steps_done / self.EPS_DECAY)\n",
    "        self.currentEPS=eps_threshold\n",
    "        self.steps_done+=1\n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                # t.max(1) will return the largest column value of each row.\n",
    "                # second column on max result is index of where max element was\n",
    "                # found, so we pick action with the larger expected reward.\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randint(0,4)]], device=device, dtype=torch.long)\n",
    "\n",
    "    def plot_durations(self,show_result=False):\n",
    "        plt.figure(1)\n",
    "        durations_t = torch.tensor(self.episode_G, dtype=torch.float)\n",
    "        if show_result:\n",
    "            plt.title('Result:')\n",
    "        else:\n",
    "            plt.clf()\n",
    "            plt.title('Training...:%.3f'%self.currentEPS)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('G')\n",
    "        plt.plot(durations_t.numpy())\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            plt.plot(means.numpy())\n",
    "\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                display.display(plt.gcf())\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory)<self.BATCH_SIZE:\n",
    "            return\n",
    "        transitions=self.memory.sample(self.BATCH_SIZE)\n",
    "        batch=Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask=torch.tensor(tuple(map(lambda s: s is not None,batch.next_state)),device=device,dtype=torch.bool)\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "        state_action_values=self.policy_net(state_batch).gather(1,action_batch)\n",
    "\n",
    "        next_state_values=torch.zeros(self.BATCH_SIZE,device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        # Compute the expected Q values\n",
    "        expected_state_action_values = (next_state_values * self.GAMMA) + reward_batch\n",
    "\n",
    "        criterion=nn.SmoothL1Loss()\n",
    "        loss=criterion(state_action_values,expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def act(self, state):\n",
    "        # Simple-minded agent that always select action 1\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "341de44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringStateToIntState(state):\n",
    "    tyreDict={}\n",
    "    tyreDict[\"Ultrasoft\"]=0\n",
    "    tyreDict[\"Soft\"]=1\n",
    "    tyreDict[\"Intermediate\"]=2\n",
    "    tyreDict[\"Fullwet\"]=3\n",
    "\n",
    "    weatherDict={}\n",
    "    weatherDict[\"Dry\"]=0\n",
    "    weatherDict[\"20% Wet\"]=1\n",
    "    weatherDict[\"40% Wet\"]=2\n",
    "    weatherDict[\"60% Wet\"]=3\n",
    "    weatherDict[\"80% Wet\"]=4\n",
    "    weatherDict[\"100% Wet\"]=5\n",
    "\n",
    "    return([tyreDict[state[0]],state[1],weatherDict[state[2]],state[3],state[4]])\n",
    "\n",
    "\n",
    "\n",
    "class Car:\n",
    "    def __init__(self, tyre=\"Intermediate\"):\n",
    "        self.default_tyre = tyre\n",
    "        self.possible_tyres = [\"Ultrasoft\", \"Soft\", \"Intermediate\", \"Fullwet\"]\n",
    "        self.pitstop_time = 23\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.change_tyre(self.default_tyre)\n",
    "    \n",
    "    \n",
    "    def degrade(self, w, r):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            self.condition *= (1 - 0.0050*w - (2500-r)/90000)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            self.condition *= (1 - 0.0051*w - (2500-r)/93000)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            self.condition *= (1 - 0.0052*abs(0.5-w) - (2500-r)/95000)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            self.condition *= (1 - 0.0053*(1-w) - (2500-r)/97000)\n",
    "        \n",
    "        \n",
    "    def change_tyre(self, new_tyre):\n",
    "        assert new_tyre in self.possible_tyres\n",
    "        self.tyre = new_tyre\n",
    "        self.condition = 1.00\n",
    "    \n",
    "    \n",
    "    def get_velocity(self):\n",
    "        if self.tyre == \"Ultrasoft\":\n",
    "            vel = 80.7*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Soft\":\n",
    "            vel = 80.1*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Intermediate\":\n",
    "            vel = 79.5*(0.2 + 0.8*self.condition**1.5)\n",
    "        elif self.tyre == \"Fullwet\":\n",
    "            vel = 79.0*(0.2 + 0.8*self.condition**1.5)\n",
    "        return vel\n",
    "\n",
    "    \n",
    "class Track:\n",
    "    def __init__(self, car=Car()):\n",
    "        # self.radius and self.cur_weather are defined in self.reset()\n",
    "        self.total_laps = 2\n",
    "        self.car = car\n",
    "        self.possible_weather = [\"Dry\", \"20% Wet\", \"40% Wet\", \"60% Wet\", \"80% Wet\", \"100% Wet\"]\n",
    "        self.wetness = {\n",
    "            \"Dry\": 0.00, \"20% Wet\": 0.20, \"40% Wet\": 0.40, \"60% Wet\": 0.60, \"80% Wet\": 0.80, \"100% Wet\": 1.00\n",
    "        }\n",
    "        self.p_transition = {\n",
    "            \"Dry\": {\n",
    "                \"Dry\": 0.987, \"20% Wet\": 0.013, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"20% Wet\": {\n",
    "                \"Dry\": 0.012, \"20% Wet\": 0.975, \"40% Wet\": 0.013, \"60% Wet\": 0.000, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"40% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.012, \"40% Wet\": 0.975, \"60% Wet\": 0.013, \"80% Wet\": 0.000, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"60% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.012, \"60% Wet\": 0.975, \"80% Wet\": 0.013, \"100% Wet\": 0.000\n",
    "            },\n",
    "            \"80% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.012, \"80% Wet\": 0.975, \"100% Wet\": 0.013\n",
    "            },\n",
    "            \"100% Wet\": {\n",
    "                \"Dry\": 0.000, \"20% Wet\": 0.000, \"40% Wet\": 0.000, \"60% Wet\": 0.000, \"80% Wet\": 0.012, \"100% Wet\": 0.988\n",
    "            }\n",
    "        }\n",
    "        self.reset()\n",
    "    \n",
    "    \n",
    "    def reset(self):\n",
    "        self.radius = 1200#np.random.randint(600,1201)\n",
    "        self.cur_weather = np.random.choice(self.possible_weather)\n",
    "        self.is_done = False\n",
    "        self.pitstop = False\n",
    "        self.laps_cleared = 0\n",
    "        self.car.reset()\n",
    "        return self._get_state()\n",
    "    \n",
    "    \n",
    "    def _get_state(self):\n",
    "        return [self.car.tyre, self.car.condition, self.cur_weather, self.radius, self.laps_cleared]\n",
    "        \n",
    "    \n",
    "    def transition(self, action=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            action (int):\n",
    "                0. Make a pitstop and fit new ‘Ultrasoft’ tyres\n",
    "                1. Make a pitstop and fit new ‘Soft’ tyres\n",
    "                2. Make a pitstop and fit new ‘Intermediate’ tyres\n",
    "                3. Make a pitstop and fit new ‘Fullwet’ tyres\n",
    "                4. Continue the next lap without changing tyres\n",
    "        \"\"\"\n",
    "        ## Pitstop time will be added on the first eight of the subsequent lap\n",
    "        time_taken = 0\n",
    "        if self.laps_cleared == int(self.laps_cleared):\n",
    "            if self.pitstop:\n",
    "                self.car.change_tyre(self.committed_tyre)\n",
    "                time_taken += self.car.pitstop_time\n",
    "                self.pitstop = False\n",
    "        \n",
    "        ## The environment is coded such that only an action taken at the start of the three-quarters mark of each lap matters\n",
    "        if self.laps_cleared - int(self.laps_cleared) == 0.75:\n",
    "            if action < 4:\n",
    "                self.pitstop = True\n",
    "                self.committed_tyre = self.car.possible_tyres[action]\n",
    "            else:\n",
    "                self.pitstop = False\n",
    "        \n",
    "        self.cur_weather = np.random.choice(\n",
    "            self.possible_weather, p=list(self.p_transition[self.cur_weather].values())\n",
    "        )\n",
    "        # we assume that degration happens only after a car has travelled the one-eighth lap\n",
    "        velocity = self.car.get_velocity()\n",
    "        time_taken += (2*np.pi*self.radius/8) / velocity\n",
    "        reward = 30-time_taken\n",
    "        self.car.degrade(\n",
    "            w=self.wetness[self.cur_weather], r=self.radius\n",
    "        )\n",
    "        self.laps_cleared += 0.125\n",
    "        \n",
    "        if self.laps_cleared == self.total_laps:\n",
    "            self.is_done = True\n",
    "        \n",
    "        next_state = self._get_state()\n",
    "        return reward, next_state, self.is_done, velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9bc5180",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_car = Car()\n",
    "env = Track(new_car)\n",
    "\n",
    "agent = Agent(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddc6d6a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 253\n"
     ]
    }
   ],
   "source": [
    "state = env.reset()    \n",
    "done = False\n",
    "G = 0\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    reward, next_state, done, velocity = env.transition(action)\n",
    "    # added velocity for sanity check\n",
    "    state = deepcopy(next_state)\n",
    "    G += reward\n",
    "\n",
    "print(\"G: %d\" % G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90067d6c",
   "metadata": {},
   "source": [
    "### Race 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c74f38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store historical weather\n",
    "holder = []\n",
    "\n",
    "state = env.reset()\n",
    "start_state = deepcopy(state)\n",
    "done = False\n",
    "G = 0\n",
    "while not done:\n",
    "    holder.append(env.cur_weather)\n",
    "    action = agent.act(state)\n",
    "    reward, next_state, done, velocity = env.transition(action)\n",
    "    # added velocity for sanity check\n",
    "    state = deepcopy(next_state)\n",
    "    G += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e572da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "group1 = Agent(5,5)\n",
    "group2 = Agent(5,5)\n",
    "group3 = Agent(5,5)\n",
    "group4 = Agent(5,5)\n",
    "group5 = Agent(5,5)\n",
    "group6 = Agent(5,5)\n",
    "group7 = Agent(5,5)\n",
    "group8 = Agent(5,5)\n",
    "group9 = Agent(5,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46100663",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/fustcianshan/repo/CS609-Project/DQN.ipynb 单元格 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m t\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m     env\u001b[39m.\u001b[39mcur_weather \u001b[39m=\u001b[39m holder[i]   \u001b[39m# assert weather transition\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m     action_tensor\u001b[39m=\u001b[39magent\u001b[39m.\u001b[39mselect_action(state_tensor)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bcs612_project/home/fustcianshan/repo/CS609-Project/DQN.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m     reward,next_state,terminated,velocity\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39mtransition(action_tensor\u001b[39m.\u001b[39mitem())\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_episodes=1000\n",
    "\n",
    "agent=Agent(5,5)\n",
    "\n",
    "for i_episode in range(num_episodes):\n",
    "        start_weather, radius = start_state[2], start_state[3]\n",
    "\n",
    "    #for agent in [group1]:#, group2, group3, group4, group5, group6, group7, group8, group9]:\n",
    "        state = env.reset() \n",
    "        state_tensor=torch.tensor(stringStateToIntState(state),dtype=torch.float32,device=device).unsqueeze(0)\n",
    "        env.cur_weather = start_weather   # assert common start weather\n",
    "        env.radius = radius               # assert common track radius\n",
    "        done = False\n",
    "        G = 0\n",
    "        i = 0\n",
    "        t=0\n",
    "        while True:\n",
    "            env.cur_weather = holder[i]   # assert weather transition\n",
    "\n",
    "            action_tensor=agent.select_action(state_tensor)\n",
    "\n",
    "            reward,next_state,terminated,velocity=env.transition(action_tensor.item())\n",
    "            #print(action_tensor.item())\n",
    "            #print(reward)\n",
    "            reward_tensor=torch.tensor([reward],device=device)\n",
    "            done=terminated\n",
    "\n",
    "            # added velocity for sanity check\n",
    "            G += reward\n",
    "            i += 1\n",
    "\n",
    "            if terminated:\n",
    "                next_state_tensor=None\n",
    "            else:\n",
    "                next_state_tensor=torch.tensor(stringStateToIntState(next_state),dtype=torch.float32,device=device).unsqueeze(0)\n",
    "\n",
    "            agent.memory.push(state_tensor,action_tensor,next_state_tensor,reward_tensor)\n",
    "\n",
    "            state = deepcopy(next_state)\n",
    "            state_tensor=torch.tensor(stringStateToIntState(state),dtype=torch.float32,device=device).unsqueeze(0)\n",
    "\n",
    "            agent.optimize_model()\n",
    "            target_net_state_dict = agent.target_net.state_dict()\n",
    "            policy_net_state_dict = agent.policy_net.state_dict()\n",
    "            for key in policy_net_state_dict:\n",
    "                target_net_state_dict[key] = policy_net_state_dict[key]*agent.TAU + target_net_state_dict[key]*(1-agent.TAU)\n",
    "            agent.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "            if done and i_episode%10==0:\n",
    "                agent.episode_G.append(G)\n",
    "                agent.plot_durations()\n",
    "                break\n",
    "            \n",
    "        \n",
    "        #print(\"G: %.2f\" % G)\n",
    "\n",
    "print('Complete')\n",
    "group1.plot_durations(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b44ef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
